#!/usr/bin/env python
# -*- coding: utf-8 -*-

#########################################
# Applied Econometrics, PS2             #
# Rudy Gilman                           #
# March 16, 2016                        #
#########################################

import pandas as pd
import numpy as np
from pandas import DataFrame
import statsmodels.formula.api as smf
import statsmodels.api as sm
import matplotlib.pyplot as plt
import os, copy
from causalinference import CausalModel
from scipy import stats
from scipy.stats import ttest_ind
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import cross_val_score
import seaborn as sns

path = "/home/rudebeans/Desktop/school_spring2016/applied_econometrics/"

#path = os.path.dirname(__file__)

df = pd.read_stata(path+"smoking2.dta", convert_categoricals=False)
#df = df.sample(n=20000, random_state=1985)
df['intercept'] = 1 # Adding constant

original = copy.deepcopy(df)

"""
This exercise examines the following research question: What is the effect of maternal smoking during pregnancy on infant birth weight and death? Feel free to work cooperatively and in groups. Each person must hand in his/her own problem set using his/her own words and interpretation of the results. Please include a concise summary of your empirical results when appropriate.

We will analyze the following STATA data set:

Data Source: smoking2.dta
This STATA data extract is from the 1989 Linked National Natality-Mortality Detail Files, which are an
annual census of births in the U.S., derived from Certificates of Live Birth. Information on subsequent infant death within a year of birth is derived from Death Certificates. This extract consists of all births in Pennsylvania in 1989. The observational unit of the data is the mother-infant outcome match.

Data Notes:

1. There are 139,149 observations and 32 variables. For this problem set, observations with missing values for any of the variables below were dropped from the original sample (about 17%).

2. The key variables are:
dbirwt = birth weight of the infant (in grams)
death = indicator equal to one if the infant died within one-year of birth and zero, otherwise
tobacco = indicator equal to one if the mother smoked during pregnancy and zero, otherwise.

3. The relevant control variables are:

Mother’s Attributes:
dmage (mother’s age), dmeduc (mother’s educational attainment), mblack (indicator=1 if mother is black), motherr (=1 if neither black nor white), mhispan (=1 if Hispanic), dmar (=1 if mother is unmarried), foreignb (=1 if mother is foreign born)

Father’s Attributes:
dfage (father’s age), dfeduc (father’s education), fblack, fotherr, fhispan (racial indicators for father)

Other Risky Behavior:
alcohol (indicator=1 if mother drank alcohol during pregnancy), drink (# of drinks per week)

Medical Care:
tripre1, tripre2, tripre3 (indicators=1 if 1st prenatal care visit in 1st, 2nd, or 3rd trimester, respectively), tripre0 (=1 if no prenatal care visits), nprevist (total # of prenatal care visits)

Pregnancy History and Maternal Health:
first (=1 if first-born), dlivord (birth order), deadkids (# previous births where newborn died), disllb (months since last birth), preterm (=1 if previous birth premature or small for gestational age), pre4000 (=1 if previously had > 4000 gram newborn), plural (=1 if twins or greater birth), phyper (=1 if mother had pregnancy-associated hypertension), diabete (=1 if mother diabetic), anemia (=1 if mother anemic)

Questions:
"""
print("\n\n\n a) Under what conditions can one identify the average treatment effect of maternal smoking by comparing the unadjusted difference in mean birth weight of infants of smoking and non-smoking mothers? Under the assumption that maternal smoking is randomly assigned, estimate its impact on birth weight. Provide some evidence for or against the hypothesis that maternal smoking is randomly assigned.")

print("\n\n\n Answer: If smoking is randomly assigned. Evidence against random assignment: Mothers who smoke probably do other things to lower birthweight, i.e. drink alcohol, do drugs. As we'll see below, this is the case. Evidence for random assignment: If these results were from a society / time period in which smoking wasn't known to be unhealthy, assignment might be random. As the means for smokers and nonsmokers shown below demonstrates, smokers and nonsmokers are very different in every way measured.\n\n\n")

# I'm putting many of the answers in functions to avoid having to repeat them all in later questions.
def do_a(df, variable):
    df_smoke = df[df.tobacco == 1]
    avgSmoke = np.mean(df_smoke[variable])

    df_noSmoke = df[df.tobacco == 0]
    avgNoSmoke = np.mean(df_noSmoke[variable])

    smokeImpact = avgSmoke - avgNoSmoke
    print("Estimated impact on "+str(variable)+" is {}".format(smokeImpact))

    t = ttest_ind(df_noSmoke[variable], df_smoke[variable])
    print("Diff means t-test: {}".format(t))
    
    mean_smokers = []
    mean_nonsmokers = []
    ts = []
    for i in range(len(df.columns)):
        var = df.columns[i]
        m_s = np.mean(df_smoke[var])
        mean_smokers.append(m_s)
        m_ns = np.mean(df_noSmoke[var])
        mean_nonsmokers.append(m_ns)
        t = ttest_ind(df_noSmoke[var], df_smoke[var])[0]
        ts.append(t)
    means = pd.DataFrame({'mean_smokers':mean_smokers, 'mean_nonSmokers':mean_nonsmokers, 't_test':ts, 'variable':df.columns})
    print("\n\n\nComparing means in smoking and control groups\n\n\n")
    
    print means
    
do_a(df, 'dbirwt')

print("\n\n\n b) Suppose that maternal smoking is randomly assigned conditional on the other observable determinants of infant birth weight. What does this imply about the relationship between maternal smoking and unobservable determinants of birth weight conditional on the observables? Use a basic linear regression model to estimate the impact of smoking and report your estimates. Under what conditions is the average treatment effect identified?\n\n\n")

def do_b(df, variable):
    y = df[variable] 
    global X
    X = df.loc[:,df.columns[df.columns.isin(['dbirwt','death', 'p','pg','weights'])==False]]
    lm = sm.OLS(y, X).fit()
    for_later_params = lm.params
    print("\n\n\nusing ols to estimate impact of smoking on {}\n\n\n".format(variable))
    print(lm.summary())

do_b(df, 'dbirwt')

print("\n\n\n Answer: Then unobserved determinants of birwt would be uncorrelated w tobacco conditional on the variables. As our linear model shows, effect of smoking reduced. There is a positive correlation between smoking and other factors that reduce birthweight. Average treatment effect IDed if treatment effects are homogenous. \n\n\n")
#TODO: check

print("\n\n\n c) Under the assumption of random assignment conditional on the observables, what are the sources of misspecification bias in the estimates generated by the linear model estimated in b)? Now use an approach in the spirit of multivariate matching. In other words, estimate the smoking effects using a flexible functional form for the control variables (e.g., higher order terms and interactions; include lots of controls as you would if you were estimating the propensity score, but just include them as controls in your regression). What are the benefits and drawbacks to this approach?\n\n\n")

print("\n\n\n Answer: Misspecification bias in that we're not accounting for higher-order and interactive terms. Effect of smoking now reduced further. Benefits are we're controlling for higher order terms and interactions, reducing bias in our estimated effects. Drawbacks are the cumbersome nature of dragging around all these control variables, increase in the variance of our variable of interest (curse of dimensionality), and the fact that they somewhat distract from our true area of interest--smoking. \n\n\n") 


print("Note: We're using a random forest classifier to draw up a list of the most important variables with regards to predicting dbirwt. The most important variables will be interacted with eachother and used to make higher-order terms. \n\n\n") 

# Returns df with variables and importance, descending
def get_imp(X,y):
    rf = RandomForestClassifier(criterion='entropy', min_samples_split=40, n_estimators=50, random_state=99, max_depth=10)
    rf.fit(X, y)
    imp_var = rf.feature_importances_
    imp_var = pd.DataFrame({'variable':X.columns, 'imp':imp_var}).sort('imp', ascending=False)
    return(imp_var)
 
imp_var=get_imp(X, df.dbirwt)    
sig = list(imp_var[imp_var.imp > .001].variable)
sig.remove('tobacco')

for_interact = df[df.columns[df.columns.isin(sig)==True]]

# returns df w only interacted, higher-order variables
def get_interact_higher(df):
    # Interacting each significant variable with every other significant variable
    for i in range(len(sig)): 
        for r in range(len(sig)):
            if i < r:
                colName = str(sig[i]) + str(sig[r])
                df[colName] = df[sig[i]] * df[sig[r]]                           
    # Making higher order variables up to ^4
    for i in range(len(sig)): 
        for r in range(2,5):
            df[str(sig[i])+str(r)] = (df[sig[i]])**r
    dup = []
    for i in range(len(df.columns)): 
        # Deleting identical cols
        for r in range(len(df.columns)):
            if (i < r) and (list(df[df.columns[i]]) == list(df[df.columns[r]])):
                dup.append(df.columns[i])  
        # Deleting columns w no variation
        if np.std(df[df.columns[i]]) == 0:
            print df.columns[i]
            dup.append(df.columns[i])   
    return df

interacted = get_interact_higher(for_interact)

# Choosing most important of interacted, higher order variables
d = get_imp(interacted, df.dbirwt)

sig = d[d.imp > 0.01].variable
interacted = interacted[interacted.columns[interacted.columns.isin(sig)==True]]

# This df contains all original values + all extras relevent to predicting dbirwt
df_full_ols = pd.concat([df, interacted], axis = 1)
df_full_ols['intercept'] = 1
df_full_ols.to_pickle('df_full_ols.txt')

df_full_ols = pd.read_pickle('df_full_ols.txt')

# Running ols again w interacted, higher-order controls
X = df_full_ols[df_full_ols.columns[df_full_ols.columns.isin(['dbirwt', 'death'])==False]]
lm = sm.OLS(df_full_ols.dbirwt, X).fit()
print(lm.summary())

print("\n\n\n d) Describe the propensity score approach to the problem of estimating the average causal effect of smoking when the treatment is randomly assigned conditional on the observables. How does it reduce the dimensionality “problem” of multivariate matching?\n\n\n") 

print("\n\n\n Answer: By taking all the control variables and turning them into a composite index indicating an individual's liklihood of being a smoker. Now we don't have to match on all the previous control variables, just on this index. \n\n\n")

print("\n\n\n e) Implement the propensity score approach to the evaluation problem using two methods: 1) control directly for the estimated propensity scores in a regression model; 2) use the estimated propensity score in a classification scheme to “stratify” the sample. Provide empirical evidence that your implementation is reasonable and evidence on the overlap of the observables of smokers and nonsmokers.  Present your findings and interpret the results. (This is an open-ended question, so show me what you know and be thoughtful).\n\n\n")

print("Note: We're using a random forest classifier again to draw up a list of the most important variables with regards to identifying smokers. The most important variables will be interacted with eachother and used to make higher-order terms. We'll use a logistic regression to estimate liklihood of smoking.  \n\n\n") 

# not using these to predict smoking
kill = ['tobacco', 'dbirwt', 'death'] 
X_lg = df[df.columns[df.columns.isin(kill) == False]]

# Finding + interacting most important variables for predicting tobacco
imp_var = get_imp(X_lg,df.tobacco)
sig = list(imp_var[imp_var.imp > .01].variable)
for_interact_lg = df[df.columns[df.columns.isin(sig)==True]]
interacted_lg = get_interact_higher(for_interact_lg)

# Choosing most important of interacted, higher order variables
d = get_imp(interacted_lg, df.tobacco)
sig = d[d.imp > 0.01].variable
interacted_lg = interacted_lg[interacted_lg.columns[interacted_lg.columns.isin(sig)==True]]

# This dataset contains all original variables + all extras relevent to predicting tobacco
df_full_lg = pd.concat([df, interacted_lg], axis = 1)
df_full_lg['intercept'] = 1
df_full_lg.to_pickle('df_full_lg.txt')

df_lg = pd.read_pickle('df_full_lg.txt')
kill = ['tobacco', 'dbirwt', 'death'] 
X = df_lg[df_lg.columns[df_lg.columns.isin(kill) == False]]
y = df_lg.tobacco

lg = LogisticRegression(random_state=3)
lg.fit(X,y)

pred = lg.predict_proba(X)
pred = [e[1] for e in pred]

# Predicted logit values (propensity score), adding back to ORIGINAL df
df['p'] = pred

# This ugly function below finds the optimal p-score groups. On each pass, it splits the groups that don't satisfy the threshhold criterion set.
def optimize_propensity_groups(df, threshhold):

    def assign_groups(df):
        num=2 
        ls = sorted(list(df.p))
        l = len(ls)/num

        def get_group(x):
            for i in range(num):
                lower = ls[l*i]
                if x >= lower and x <= ls[l*i+l-1]:
                    return lower             
        p = df.p           
        pg = p.map(get_group)
        df['pg'] = pg
        return df

    df = assign_groups(df) # First assignment of groups

    def get_scores(df):
        tots=[]
        gs = sorted(df.pg.unique())
        for i in range(len(df.pg.unique())):
            groupId = gs[i]
            d = df[df.pg==groupId]
            d_smoke = d[d.tobacco==1]
            d_noSmoke = d[d.tobacco==0]     
            gScore = 0 
            for c in range(len(d.columns)):
                var = d.columns[c]
                if var not in ['pg','tobacco']:      
                    try:
                        t = ttest_ind(d_smoke[var], d_noSmoke[var])[0]
                    except:
                        t = 2      
                    if np.abs(t) < 1.96:
                        gScore += 1            
            groupScore = float(gScore) / float((len(d.columns)-2))
            tots.append((groupId, groupScore))
        return(tots)

    def update_groups(df): # Performs one update pass across groups
        scores = get_scores(df)
        f = pd.DataFrame({})
        for i in range(len(scores)):
            groupId = scores[i][0]
            s = df[df.pg==groupId]
            if scores[i][1] < threshhold:
                s = df[df.pg==groupId]
                s = assign_groups(s)
                f=pd.concat([f, s], axis=0)
            else:
                f=pd.concat([f, s], axis=0)
        return f
                   
    def calc_final_score(df):
        scores = get_scores(df)
        groupNames = [i[0] for i in scores]
        groupScores = [i[1] for i in scores]
        match = pd.Series(groupScores)
        match = match[match>=threshhold]
        finalScore = float(len(match)) / float(len(groupScores))
        return finalScore
        
    while calc_final_score(df) < threshhold: # Updating groups until threshhold met
        df = update_groups(df)

    print(calc_final_score(df))
    print(len(get_scores(df)))   
    print len(df.pg.unique())

    return df

df = optimize_propensity_groups(df, 0.7)

df.to_pickle('df.txt')

# This dataframe has p-scores and p-groups
df = pd.read_pickle('df.txt')
df_y = df[(df.dmage<=32)&(df.dmage>=16)]

# Trimming off outliers (top and bottom .10)
def trim(df, amount):
    df = df.dropna().sort_values(['p'])
    tenPerc = int(len(df)/(100.0/float(amount)))
    minP = df.iloc[tenPerc]['p']
    maxP = df.iloc[len(df)-tenPerc]['p']
    df = df[(df.p>minP)&(df.p<maxP)]
    return df
    
df = trim(df, .10)
df_y = trim(df_y, .10)

print("\n\n\n Using propensity score as control in original regression. \n\n\n")
y = df.dbirwt
pX = df.loc[:,['p', 'tobacco', 'intercept']]
lm_p = sm.OLS(y,pX).fit()
print(lm_p.summary())

# Repeating for young dataset
y = df_y.dbirwt
pX = df_y.loc[:,['p', 'tobacco', 'intercept']]
lm_p = sm.OLS(y,pX).fit()
# Results moved to later question
#print('\n\n\n YOUNG:\n\n\n')
#print(lm_p.summary())

df_smoke = df[df.tobacco == 1]
df_noSmoke = df[df.tobacco == 0]

# Boxplot
print("\n\n\n Showing overlap btwn our groups. There's overlap, but we're clearly dealing with two very different groups here \n\n\n")
ax = sns.boxplot(x='tobacco', y='p', data=df)
ax.set_title('pscore overlap: boxplot')
sns.plt.show()
ax.cla()

# Histogram: legend
fig = plt.figure()
ax = fig.add_subplot(111)
h1 = ax.hist(list(df_smoke.p), bins=30, normed=True, alpha=.5)
h2 = ax.hist(list(df_noSmoke.p), bins=30, normed=True, alpha=.5)
ax.set_title('p-score overlap: kdensity')
ax.set_xlabel('estimated propensity')
ax.set_ylabel('normalized frequency')
ax.legend(['smokers', 'nonsmokers'])
plt.show()
fig.clf()


ks = stats.ks_2samp(df_smoke.p, df_noSmoke.p)
print("\n\n The kalashnikov smirnoff test shows that our p-score distributions are very much not the same: {}\n\n".format(ks))

# Matching by p-score group and comparing TEs
def compare_pg(df):
    print("\n\nMatching by p-score group and comparing TEs\n\n")
    pg_strat = pd.DataFrame(columns=['groupName','TE','TE_tscore','ks'])
    for i in range(len(df.pg.unique())):
        pg = df.pg.unique()[i]
        d = df[df.pg==pg]
        d_smoke = d[d.tobacco==1]
        d_noSmoke = d[d.tobacco==0]
        try:
            t = ttest_ind(d_smoke.dbirwt, d_noSmoke.dbirwt)[0]
        except:
            t = np.nan
        TE = np.mean(d_smoke.dbirwt) - np.mean(d_noSmoke.dbirwt)
        try:
            ks = stats.ks_2samp(d_smoke.p, d_noSmoke.p)
        except: 
            ks = np.nan
        pg_strat = pg_strat.append({'groupName':pg, 'TE':TE, 'TE_tscore':t, 'ks':ks}, ignore_index=True)
    return pg_strat
   
print("As we can see in the classification table below, this approach was reasonable: In the majority of our groups, we can't reject the KS-test null that the p-score distributions are the same. As we can see in the box plot above, we started with decent overlap btwn the smokers and controls.\n\n\n" )   
   
print(compare_pg(df))

print("\n\n\nf) Now use the estimated propensity scores to reweigh the outcomes and estimate: i) the population average treatment effect; and ii) the average treatment effect among the treated. Compare your estimates to those in e) and interpret your findings. What are the benefits and drawbacks of approaches that use the estimated propensity scores as weights?\n\n\n")

# getting tot effect:
def do_f(df):

    print("\n\n\n Finding TOT effects \n\n\n")
    X = df.loc[:, df.columns[df.columns.isin(['dbirwt','tobacco','death'])==False]]
    df['weights'] = 1
    df_smoke = df[df.tobacco==1]
    df_noSmoke = df[df.tobacco==0]
    df_noSmoke.weights = df_noSmoke.weights * (df_noSmoke.p / (1.0-df_noSmoke.p) )
    df = pd.concat([df_smoke, df_noSmoke], axis=0)
    x = df.loc[:,['tobacco','intercept']]
    wls = sm.WLS(df.dbirwt, x, weights=df.weights).fit()
    print(wls.summary())
    
    # Histogram: legend
    fig = plt.figure()
    ax = fig.add_subplot(111)
    h1 = ax.hist(list(df_smoke.p), weights=list(df_smoke.weights), bins=30, normed=True, alpha=.5)
    h2 = ax.hist(list(df_noSmoke.p), weights=list(df_noSmoke.weights), bins=30, normed=True, alpha=.5)
    ax.set_title('TOT Weighted p-score distribution by treatment')
    ax.set_xlabel('estimated propensity')
    ax.set_ylabel('normalized frequency')
    ax.legend(['smokers', 'nonsmokers'])
    plt.show()
    fig.clf()
    
    # getting ATE:
    print("\n\n\n Finding ATE effects \n\n\n")
    X = df.loc[:, df.columns[df.columns.isin(['dbirwt','tobacco','death'])==False]]
    df['weights'] = 1
    df_smoke = df[df.tobacco==1]
    df_noSmoke = df[df.tobacco==0]
    df_noSmoke.weights = (df_noSmoke.p / (1.0-df_noSmoke.p) )
    df_smoke.weights =  (1 / df_smoke.p)
    df = pd.concat([df_smoke, df_noSmoke], axis=0)
    x = df.loc[:,['tobacco','intercept']]
    wls = sm.WLS(df.dbirwt, x, weights=df.weights).fit()
    print(wls.summary())
    
    # Histogram: legend
    fig = plt.figure()
    ax = fig.add_subplot(111)
    h1 = ax.hist(list(df_smoke.p), weights=list(df_smoke.weights), bins=30, normed=True, alpha=.5)
    h2 = ax.hist(list(df_noSmoke.p), weights=list(df_noSmoke.weights), bins=30, normed=True, alpha=.5)
    ax.set_title('ATE Weighted p-score distribution by treatment')
    ax.set_xlabel('estimated propensity')
    ax.set_ylabel('normalized frequency')
    ax.legend(['smokers', 'nonsmokers'])
    plt.show()
    fig.clf()
    
do_f(df)

print("Answer: Our estimates are similar to those found in e). An advantage is that all observations can be used. \n\n\n")

print("g) A more informative way to describe the birth weight effects of smoking is to estimate the “nonparametric” conditional mean of birth weight as a function of the estimated propensity score, for smokers and non-smokers. To do this simply, stratify the smokers into 100 equal-sized cells based on their propensity scores and calculate the mean birth weight and propensity score in each cell. Do the same for the non-smokers. Plot these 2 conditional mean functions on the same graph, with the mean scores on the x-axis and mean birth weight on the y-axis. Interpret your findings and relate them to the results in e) and f). Now redo the above but use 200 equal-sized cells for smokers and non-smokers together – i.e., each cell should contain 1/200th’s of the data, with some cells containing very few smokers and other cells containing mostly smokers.")

print("\n\n\n Answer: Looks similar to what we found earlier. Relationship btwn p-score and dbirwt looks linear. Treatment effects look homogenous. Breaking p-scores into 200 groups leads to more noise, as there are some groups with very few individuals.")

# breaks df.p into equally sized groups based on percentile
def assign_groups(df, num):
    ls = sorted(list(df.p))
    l = int(len(ls)/num)
    def get_group(x):
        for i in range(num):
            lower = ls[l*i]
            if x > lower and x <= ls[l*i+l-1]:
                return lower          
    p = df.p           
    pg = p.map(get_group)
    df['pg'] = pg.fillna(np.min(pg))
    return df
    
# returns small dataframe with p-score groups and dbirwt means for graphing
def get_cond_means(df):
    means_s = []
    means_ns = []
    pg = df.pg.unique()
    for i in range(len(pg)):
        pg_i = pg[i]
        d = df[df.pg==pg_i]
        d_s = d[d.tobacco==1]
        d_ns = d[d.tobacco==0]     
        m_s = np.mean(d_s.dbirwt)
        m_ns = np.mean(d_ns.dbirwt)       
        means_s.append(m_s)
        means_ns.append(m_ns)
    cm = pd.DataFrame({'means_s':means_s, 'means_ns':means_ns, 'pg':pg})
    return cm

# For making scatterplots. y, x must both be array
def scat(x, ys, title=None, legend=None):
    c = ['blue', 'red', 'green', 'yellow', 'purple']
    fig = plt.figure()
    ax = fig.add_subplot(111)
    for i in range(len(ys)):
        ax.scatter(x, ys[i], c=c[i])
    ax.set_title(title)
    y=''
    for h in ys: y += str(h.name)+', '   
    ax.set_ylabel(str(y))
    ax.set_xlabel(str(x.name))
    ax.legend(legend)
    plt.show()
    fig.clf()

def do_g(df):
    # making scatter for 100 group    
    cm_100 = get_cond_means(assign_groups(df, 100))
    scat(cm_100.pg, [cm_100.means_ns, cm_100.means_s], 'mean birwt by p-group: 100', ['nonsmokers', 'smokers'])

    # making scatter for 200 group
    cm_200 = get_cond_means(assign_groups(df, 200))
    scat(cm_200.pg, [cm_200.means_ns, cm_200.means_s], 'mean birwt by p-group: 100', ['nonsmokers', 'smokers'])

do_g(df)

print("\n\n\n h) Low birth weight births (less than 2500 grams) are considered particularly undesirable since they comprise a large share of infant deaths. Redo g) using an indicator for low birth weight birth as the outcome of interest. Interpret your findings.\n\n\n")

print("\n\n\n Answer: Looks like smokers have a higher proportion of lwbwt infants, and that the proportion of lwbwt infants increases linearly with p-score. Again, 200 group is noisier.")

def low_bw_dummy(x):
        if x >= 2500.0:
            return 0 
        elif x < 2500.0:
            return 1                  
df['dbirwt_d'] = df.dbirwt.map(low_bw_dummy)

# returns small dataframe with p-score groups and dbirwt means for graphing
def get_lbw(df):
    lowbw_s = []
    lowbw_ns = []
    pg = df.pg.unique()
    for i in range(len(pg)):
        pg_i = pg[i]
        d = df[df.pg==pg_i]
        d_s = d[d.tobacco==1]
        d_ns = d[d.tobacco==0]     
        m_s = float(len(d_s[d_s.dbirwt_d ==1])) / float(len(d_s))
        m_ns = float(len(d_ns[d_ns.dbirwt_d ==1])) / float(len(d_ns))
        lowbw_s.append(m_s)
        lowbw_ns.append(m_ns)
    cm = pd.DataFrame({'lowbw_s':lowbw_s, 'lowbw_ns':lowbw_ns, 'pg':pg})
    return cm
      
lbw_100 = get_lbw(assign_groups(df, 100))
scat(lbw_100.pg, [lbw_100.lowbw_ns, lbw_100.lowbw_s], 'lowbwt perc of total by p-group: 100', ['nonsmokers', 'smokers'])

lbw_200 = get_lbw(assign_groups(df, 200))
scat(lbw_200.pg, [lbw_200.lowbw_ns, lbw_200.lowbw_s], 'lowbwt perc of total by p-group: 200', ['nonsmokers', 'smokers'])

print("\n\n\n i) Estimate the impact of maternal smoking on infant death using the methods in parts a), b), and g) (using 50 equal-sized cells, for smokers and non-smokers together). Interpret your findings. From your results, what might you conclude about the relationship between smoking and infant death? \n\n\n")


print("Answer: It doesn't look like there's a statistically significant relationship btwn smoking and infant death. There is, however, a clearly positive correlation btwn p-score and infant death \n\n\n")

# returns small dataframe with p-score groups and death props for graphing
def get_death(df):
    lowbw_s = []
    lowbw_ns = []
    pg = df.pg.unique()
    for i in range(len(pg)):
        pg_i = pg[i]
        d = df[df.pg==pg_i]
        d_s = d[d.tobacco==1]
        d_ns = d[d.tobacco==0]     
        m_s = float(len(d_s[d_s.death ==1])) / float(len(d_s))
        m_ns = float(len(d_ns[d_ns.death ==1])) / float(len(d_ns))
        lowbw_s.append(m_s)
        lowbw_ns.append(m_ns)
    cm = pd.DataFrame({'death_s':lowbw_s, 'death_ns':lowbw_ns, 'pg':pg})
    return cm

# finding simple diff in means in effect of smoking on death
print("Repeating a) for infant death \n\n\n")
do_a(original, 'death')

print("Repeating b) for infant death \n\n\n")
# running ols, effect of smoking on death
do_b(original, 'death')

print("Repeating g) for infant death \n\n\n")
# graphing
df50 = assign_groups(df, 50)
death_100 = get_death(df50)
scat(death_100.pg, [death_100.death_ns, death_100.death_s], 'perc infant death by p-group', ['nonsmokers', 'smokers'])

print("\n\n\n j) Smoking rates vary over the life-cycle of women. Plot the sample sizes and smoking rates by the age of the women in the sample. Now, separately for smoking and non-smoking women but on the same graph, plot the average birth weight of their infants by the age of the mother. Describe what you see.\n\n\n")

print("Answer: smoking is most popular among 19 yr olds, and becomes steadily less popular afterwards. Sample size by age group looks approximately normally distributed with mean at 27. Ppl younger than 15 and older than 40 are not well represented. Avg birthweight increases then decreases for both smokers and nonsmokers, peaking around age 30. The treatment effect differs significantly with age, with older women affected more negatively by smoking. There's significant noise amongst women younger than 15 and older than 40. \n\n\n")

pt = pd.pivot_table(df, ['tobacco'], 'dmage', aggfunc=np.mean)
scat(pt.index, [pt.tobacco], 'perc smoker by age', ['all'])

pt = pd.pivot_table(df, ['intercept'], 'dmage', aggfunc=np.sum)
scat(pt.index, [pt.intercept], 'sample size by age', ['all'])

df_s = df[df.tobacco==1]
df_ns = df[df.tobacco==0]
pt_s = pd.pivot_table(df_s, ['dbirwt'], 'dmage', aggfunc=np.mean)
pt_ns = pd.pivot_table(df_ns, ['dbirwt'], 'dmage', aggfunc=np.mean)

# scatter
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(pt_ns.index, pt_ns.dbirwt, c='blue')
ax.scatter(pt_s.index, pt_s.dbirwt, c='red')
ax.set_title('avg birthweight by mage')  
ax.set_ylabel('dbirwt')
ax.set_xlabel('age')
ax.legend(['nonsmokers', 'smokers'])
plt.show()
fig.clf()

print("\n\n\n k) For the sample of women aged 16 to 32 (116,243 observations), redo parts a), b), e), f), g) and i).  How do your findings contrast with your findings from the overall sample, if at all?\n\n\n")

print(" As shown below, findings are substantively the same.\n\n\n")

df_young = original[(original.dmage<=32)&(original.dmage>=16)]

print("\n\nRepeating a) for young women \n\n\n")
# parts a, b
do_a(df_young, 'dbirwt')

print("\n\nRepeating b) for young women \n\n\n")
do_b(df_young, 'dbirwt')

print("\n\nRepeating e) for young women \n\n\n")
# part e)
print("\n\n\n Using propensity score as control in original regression. \n\n\n")
print(lm_p.summary()) # model built in part e), just printing summary here
print(compare_pg(df_y))

print("\n\n\nRepeating f) for young women \n\n\n")
# part f)
do_f(df_y)

print("\n\nRepeating g) for young women \n\n\n")
# part g)
do_g(df_y)

print("\n\nRepeating i) for young women \n\n\n")
# part i)
# finding simple diff in means in effect of smoking on death
do_a(df_y, 'death')
# running ols, effect of smoking on death
do_b(df_y, 'death')
# graphing
df50 = assign_groups(df_y, 50)
death_100 = get_death(df50)
scat(death_100.pg, [death_100.death_ns, death_100.death_s], 'perc infant death by p-group', ['nonsmokers', 'smokers'])

print("\n\n\n l) Concisely and coherently summarize all of your findings. In this summary, describe the estimated effects of maternal smoking on infant birth weight and infant mortality and whether you think your “best” estimate of the effects of smoking is credibly identified. State why or why not.\n\n\n")

print("\n\n\n Using a few different techniques, we reliably measured the effect of smoking on birthweight at around -200 g. As shown in the boxplot and kdensity plots earlier, however, smokers and nonsmokers are very different in a lot of ways. We tried to control for this, and did to some extent, but the question still remains: Can we interpret this effect as causal? In my opinion, we've shown an upper bound for what the effect of smoking on dbirwt might be--the true effects may be much lower. \n\n\n")

