#!/usr/bin/env python
# -*- coding: utf-8 -*-

#########################################
# Applied Econometrics, PS1             #
# Rudy Gilman                           #
# Feb 20, 2016                          #
#########################################

import pandas as pd
import numpy as np
from pandas import DataFrame
import statsmodels.formula.api as smf
import statsmodels.api as sm
import matplotlib.pyplot as plt
import os, copy
from causalinference import CausalModel
from scipy import stats
from scipy.stats import ttest_ind
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

path = "/home/rudebeans/Desktop/school_spring2016/applied_econometrics/"

#os.path.dirname(__file__)

df = pd.read_stata(path+"smoking2.dta", convert_categoricals=False)
df = df.sample(n=20000, random_state=1985)
df['intercept'] = 1 # Adding constant

original = copy.deepcopy(df)

"""
This exercise examines the following research question: What is the effect of maternal smoking during pregnancy on infant birth weight and death? Feel free to work cooperatively and in groups. Each person must hand in his/her own problem set using his/her own words and interpretation of the results. Please include a concise summary of your empirical results when appropriate.

We will analyze the following STATA data set:

Data Source: smoking2.dta
This STATA data extract is from the 1989 Linked National Natality-Mortality Detail Files, which are an
annual census of births in the U.S., derived from Certificates of Live Birth. Information on subsequent infant death within a year of birth is derived from Death Certificates. This extract consists of all births in Pennsylvania in 1989. The observational unit of the data is the mother-infant outcome match.

Data Notes:

1. There are 139,149 observations and 32 variables. For this problem set, observations with missing values for any of the variables below were dropped from the original sample (about 17%).

2. The key variables are:
dbirwt = birth weight of the infant (in grams)
death = indicator equal to one if the infant died within one-year of birth and zero, otherwise
tobacco = indicator equal to one if the mother smoked during pregnancy and zero, otherwise.

3. The relevant control variables are:

Mother’s Attributes:
dmage (mother’s age), dmeduc (mother’s educational attainment), mblack (indicator=1 if mother is black), motherr (=1 if neither black nor white), mhispan (=1 if Hispanic), dmar (=1 if mother is unmarried), foreignb (=1 if mother is foreign born)

Father’s Attributes:
dfage (father’s age), dfeduc (father’s education), fblack, fotherr, fhispan (racial indicators for father)

Other Risky Behavior:
alcohol (indicator=1 if mother drank alcohol during pregnancy), drink (# of drinks per week)

Medical Care:
tripre1, tripre2, tripre3 (indicators=1 if 1st prenatal care visit in 1st, 2nd, or 3rd trimester, respectively), tripre0 (=1 if no prenatal care visits), nprevist (total # of prenatal care visits)

Pregnancy History and Maternal Health:
first (=1 if first-born), dlivord (birth order), deadkids (# previous births where newborn died), disllb (months since last birth), preterm (=1 if previous birth premature or small for gestational age), pre4000 (=1 if previously had > 4000 gram newborn), plural (=1 if twins or greater birth), phyper (=1 if mother had pregnancy-associated hypertension), diabete (=1 if mother diabetic), anemia (=1 if mother anemic)

Questions:

print("\n\n\n a) Under what conditions can one identify the average treatment effect of maternal smoking by comparing the unadjusted difference in mean birth weight of infants of smoking and non-smoking mothers? Under the assumption that maternal smoking is randomly assigned, estimate its impact on birth weight. Provide some evidence for or against the hypothesis that maternal smoking is randomly assigned.")

print("\n\n\n Answer: If smoking is randomly assigned. Evidence against random assignment: Mothers who smoke probably do other things to lower birthweight, i.e. drink alcohol, do drugs. Evidence for random assignment: If these results were from a society / time period in which smoking wasn't known to be unhealthy, assignment might be random.  \n\n\n")
"""
def do_a(df):
    df_smoke = df[df.tobacco == 1]
    avgSmoke = np.mean(df_smoke.dbirwt)

    df_noSmoke = df[df.tobacco == 0]
    avgNoSmoke = np.mean(df_noSmoke.dbirwt)

    smokeImpact = avgSmoke - avgNoSmoke

    print("Estimated impact on birthweight:")
    print(smokeImpact)

    print("Diff means t-test:")
    t = ttest_ind(df_noSmoke.dbirwt, df_smoke.dbirwt)
    print(t)
    
do_a(df)

print("\n\n\n b) Suppose that maternal smoking is randomly assigned conditional on the other observable determinants of infant birth weight. What does this imply about the relationship between maternal smoking and unobservable determinants of birth weight conditional on the observables? Use a basic linear regression model to estimate the impact of smoking and report your estimates. Under what conditions is the average treatment effect identified?\n\n\n")

def do_b(df):
    y = df.dbirwt 
    global X
    X = df.loc[:,df.columns[df.columns.isin(['dbirwt','death'])==False]]
    lm = sm.OLS(y, X).fit()
    for_later_params = lm.params
    print(lm.summary())

do_b(df)

print("\n\n\n Answer: Effect of smoking reduced to -217. There is a positive correlation between smoking and other factors that reduce birthweight. Average treatment effect IDed if treatment effects are homogenous. \n\n\n")
#TODO: check

print("\n\n\n c) Under the assumption of random assignment conditional on the observables, what are the sources of misspecification bias in the estimates generated by the linear model estimated in b)? Now use an approach in the spirit of multivariate matching. In other words, estimate the smoking effects using a flexible functional form for the control variables (e.g., higher order terms and interactions; include lots of controls as you would if you were estimating the propensity score, but just include them as controls in your regression). What are the benefits and drawbacks to this approach?\n\n\n")

print("\n\n\n Answer: Misspecification bias in that we're not accounting for higher-order and interactive terms. Effect of smoking reduced to -214. Benefits are we're controlling for higher order terms and interactions in a clear, open, standardizable way, reducing bias in our estimated effects. Drawbacks are the cumbersome nature of dragging around all these control variables, increase in the variance of our variable of interest, and the fact that they somewhat distract from our true area of interest--smoking. \n\n\n") 
"""
# Finding most important variables for use in interacting

# Returns df with variables and importance, descending
def get_imp(X,y):
    rf = RandomForestClassifier(criterion='entropy', min_samples_split=40, n_estimators=50, random_state=99, max_depth=10)
    rf.fit(X, y)
    imp_var = rf.feature_importances_
    imp_var = pd.DataFrame({'variable':X.columns, 'imp':imp_var}).sort('imp', ascending=False)
    return(imp_var)
 
imp_var=get_imp(X, df.dbirwt)    
sig = list(imp_var[imp_var.imp > .001].variable)
sig.remove('tobacco')

for_interact = df[df.columns[df.columns.isin(sig)==True]]

# returns df w only interacted, higher-order variables
def get_interact_higher(df):
    # Interacting each significant variable with every other significant variable
    for i in range(len(sig)): 
        for r in range(len(sig)):
            if i < r:
                colName = str(sig[i]) + str(sig[r])
                df[colName] = df[sig[i]] * df[sig[r]]                           
    # Making higher order variables up to ^4
    for i in range(len(sig)): 
        for r in range(2,5):
            df[str(sig[i])+str(r)] = (df[sig[i]])**r
    dup = []
    for i in range(len(df.columns)): 
        # Deleting identical cols
        for r in range(len(df.columns)):
            if (i < r) and (list(df[df.columns[i]]) == list(df[df.columns[r]])):
                dup.append(df.columns[i])  
        # Deleting columns w no variation
        if np.std(df[df.columns[i]]) == 0:
            print df.columns[i]
            dup.append(df.columns[i])   
    return df

interacted = get_interact_higher(for_interact)

# Choosing most important of interacted, higher order variables
d = get_imp(interacted, df.dbirwt)

sig = d[d.imp > 0.01].variable
interacted = interacted[interacted.columns[interacted.columns.isin(sig)==True]]

# This df contains all original values + all extras relevent to predicting dbirwt
df_full_ols = pd.concat([df, interacted], axis = 1)
df_full_ols['intercept'] = 1
df_full_ols.to_pickle('df_full_ols.txt')

df_full_ols = pd.read_pickle('df_full_ols.txt')

# Running ols again w interacted, higher-order controls
X = df_full_ols[df_full_ols.columns[df_full_ols.columns.isin(['dbirwt', 'death'])==False]]
#lm = sm.OLS(df_full_ols.dbirwt, X).fit()
#print(lm.summary())

print("\n\n\n d) Describe the propensity score approach to the problem of estimating the average causal effect of smoking when the treatment is randomly assigned conditional on the observables. How does it reduce the dimensionality “problem” of multivariate matching?\n\n\n") 

print("\n\n\n Answer: By taking all the control variables and turning them into a composite index indicating an individual's liklihood of being a smoker. Now we don't have to match on all the previous control variables, just on this index. \n\n\n")

print("\n\n\n e) Implement the propensity score approach to the evaluation problem using two methods: 1) control directly for the estimated propensity scores in a regression model; 2) use the estimated propensity score in a classification scheme to “stratify” the sample. Provide empirical evidence that your implementation is reasonable and evidence on the overlap of the observables of smokers and nonsmokers.  Present your findings and interpret the results. (This is an open-ended question, so show me what you know and be thoughtful).\n\n\n")

# not using these to predict smoking
kill = ['tobacco', 'dbirwt', 'death'] 
X_lg = df[df.columns[df.columns.isin(kill) == False]]

# Finding + interacting most important variables
imp_var = get_imp(X_lg,df.tobacco)
sig = list(imp_var[imp_var.imp > .01].variable)
for_interact_lg = df[df.columns[df.columns.isin(sig)==True]]
interacted_lg = get_interact_higher(for_interact_lg)

# Choosing most important of interacted, higher order variables
d = get_imp(interacted_lg, df.tobacco)
sig = d[d.imp > 0.01].variable
interacted_lg = interacted_lg[interacted_lg.columns[interacted_lg.columns.isin(sig)==True]]

# This dataset contains all original variables + all extras relevent to predicting tobacco
df_full_lg = pd.concat([df, interacted_lg], axis = 1)
df_full_lg['intercept'] = 1
df_full_lg.to_pickle('df_full_lg.txt')

df_lg = pd.read_pickle('df_full_lg.txt')
kill = ['tobacco', 'dbirwt', 'death'] 

X = df_lg[df_lg.columns[df_lg.columns.isin(kill) == False]]
y = df_lg.tobacco
lg = LogisticRegression(C=1000.0, random_state=0)

lg.fit(X,y)

pred = lg.predict_proba(X)
pred = [e[1] for e in pred]

# Predicted logit values (propensity score), adding back to ORIGINAL df
df['p'] = pred

# Using a decision tree learner to create p-score groups
dt = DecisionTreeClassifier(criterion='entropy', min_samples_split=40, random_state=99, max_depth=10)
dt.fit(X, y)
pred = dt.predict_proba(X)
pred = [e[1] for e in pred]
df['pg'] = pred

df.to_pickle('df.txt')
"""

# This dataframe has p-scores and p-groups
df = pd.read_pickle('df.txt')
df_y = df[(df.dmage<=32)&(df.dmage>=16)]

def trim(df, amount):
    df = df.dropna().sort_values(['p'])
    tenPerc = int(len(df)/(100.0/float(amount)))
    minP = df.iloc[tenPerc]['p']
    maxP = df.iloc[len(df)-tenPerc]['p']
    df = df[(df.p>minP)&(df.p<maxP)]
    return df
    
df = trim(df, .05)
df_y = trim(df_y, .05)

# Using propensity score as control in original regression
y = df.dbirwt
pX = df.loc[:,['p', 'tobacco', 'intercept']]
lm_p = sm.OLS(y,pX).fit()
print(lm_p.summary())

# Repeating for young dataset
y = df_y.dbirwt
pX = df_y.loc[:,['p', 'tobacco', 'intercept']]
lm_p = sm.OLS(y,pX).fit()
# Results moved to later question
#print('\n\n\n YOUNG:\n\n\n')
#print(lm_p.summary())

df_smoke = df[df.tobacco == 1]
df_noSmoke = df[df.tobacco == 0]

"""
# Boxplot
fig = plt.figure()
ax = fig.add_subplot(111)
ax.boxplot([list(df_smoke.p), list(df_noSmoke.p)])
ax.set_title('Prop. Score Dist. by Treatment')
ax.set_ylabel('p-score')
ax.set_xlabel('treatment')
plt.show()
fig.clf()

# Histogram: legend
fig = plt.figure()
ax = fig.add_subplot(111)
ax.hist([list(df_smoke.p), list(df_noSmoke.p)], bins=30, normed=True, alpha=.5)
ax.set_title('Estimated prop.-score distribution by treatment')
plt.legend()
ax.set_xlabel('estimated propensity')
ax.set_ylabel('normalized frequency')
plt.show()
fig.clf()
"""

ks = stats.ks_2samp(df_smoke.p, df_noSmoke.p)
print("ks: ", ks)

# Matching by p-score group and comparing TEs

def compare_pg(df):
    pg_strat = pd.DataFrame(columns=['groupName','TE','TE_tscore','ks'])
    for i in range(len(df.pg.unique())):
        pg = df.pg.unique()[i]
        d = df[df.pg==pg]
        d_smoke = d[d.tobacco==1]
        d_noSmoke = d[d.tobacco==0]
        try:
            t = ttest_ind(d_smoke.dbirwt, d_noSmoke.dbirwt)[0]
        except:
            t = np.nan
        TE = np.mean(d_smoke.dbirwt) - np.mean(d_noSmoke.dbirwt)
        try:
            ks = stats.ks_2samp(d_smoke.p, d_noSmoke.p)
        except: 
            ks = np.nan
        pg_strat = pg_strat.append({'groupName':pg, 'TE':TE, 'TE_tscore':t, 'ks':ks}, ignore_index=True)
    return pg_strat
   
print("As we can see in the classification table below, this approach was reasonable: In the majority of our groups, we can't reject the KS-test null that the p-score distributions are the same. As we can see in the box plot above, we started with decent overlap btwn the smokers and controls.\n\n\n" )   
   
print(compare_pg(df))

print("\n\n\nf) Now use the estimated propensity scores to reweigh the outcomes and estimate: i) the population average treatment effect; and ii) the average treatment effect among the treated. Compare your estimates to those in e) and interpret your findings. What are the benefits and drawbacks of approaches that use the estimated propensity scores as weights?\n\n\n")

# getting tot effect:
def do_f(df):
    X = df.loc[:, df.columns[df.columns.isin(['dbirwt','tobacco','death'])==False]]
    df['weights'] = 1
    df_smoke = df[df.tobacco==1]
    df_noSmoke = df[df.tobacco==0]
    df_noSmoke.weights = df_noSmoke.weights * (df_noSmoke.p / (1.0-df_noSmoke.p) )
    df = pd.concat([df_smoke, df_noSmoke], axis=0)
    x = df.loc[:,['tobacco','intercept']]
    wls = sm.WLS(df.dbirwt, x, weights=df.weights).fit()
    print(wls.summary())

    """
    # Histogram: legend
    fig = plt.figure()
    ax = fig.add_subplot(111)
    h1 = ax.hist(list(df_smoke.p), weights=list(df_smoke.weights), bins=30, normed=True, alpha=.5)
    h2 = ax.hist(list(df_noSmoke.p), weights=list(df_noSmoke.weights), bins=30, normed=True, alpha=.5)
    ax.set_title('TOT Weighted prop.-score distribution by treatment')
    ax.set_xlabel('estimated propensity')
    ax.set_ylabel('normalized frequency')
    plt.show()
    fig.clf()
    """

    # getting ATE:
    X = df.loc[:, df.columns[df.columns.isin(['dbirwt','tobacco','death'])==False]]
    df['weights'] = 1
    df_smoke = df[df.tobacco==1]
    df_noSmoke = df[df.tobacco==0]
    df_noSmoke.weights = (df_noSmoke.p / (1.0-df_noSmoke.p) )
    df_smoke.weights =  (1 / df_smoke.p)
    df = pd.concat([df_smoke, df_noSmoke], axis=0)
    x = df.loc[:,['tobacco','intercept']]
    wls = sm.WLS(df.dbirwt, x, weights=df.weights).fit()
    print(wls.summary())

    """
    # Histogram: legend
    fig = plt.figure()
    ax = fig.add_subplot(111)
    h1 = ax.hist(list(df_smoke.p), weights=list(df_smoke.weights), bins=30, normed=True, alpha=.5)
    h2 = ax.hist(list(df_noSmoke.p), weights=list(df_noSmoke.weights), bins=30, normed=True, alpha=.5)
    ax.set_title('ATE Weighted prop.-score distribution by treatment')
    ax.set_xlabel('estimated propensity')
    ax.set_ylabel('normalized frequency')
    plt.show()
    fig.clf()
    """
do_f(df)

print("Answer: Our estimates are similar to those found in e). \n\n\n")

print("g) A more informative way to describe the birth weight effects of smoking is to estimate the “nonparametric” conditional mean of birth weight as a function of the estimated propensity score, for smokers and non-smokers. To do this simply, stratify the smokers into 100 equal-sized cells based on their propensity scores and calculate the mean birth weight and propensity score in each cell. Do the same for the non-smokers. Plot these 2 conditional mean functions on the same graph, with the mean scores on the x-axis and mean birth weight on the y-axis. Interpret your findings and relate them to the results in e) and f). Now redo the above but use 200 equal-sized cells for smokers and non-smokers together – i.e., each cell should contain 1/200th’s of the data, with some cells containing very few smokers and other cells containing mostly smokers.")

# breaks df.p into equally sized groups based on percentile
def assign_groups(df, num):
    ls = sorted(list(df.p))
    l = int(len(ls)/num)
    def get_group(x):
        for i in range(num):
            lower = ls[l*i]
            if x > lower and x <= ls[l*i+l-1]:
                return lower          
    p = df.p           
    pg = p.map(get_group)
    df['pg'] = pg.fillna(np.min(pg))
    return df
    
# returns small dataframe with p-score groups and dbirwt means for graphing
def get_cond_means(df):
    means_s = []
    means_ns = []
    pg = df.pg.unique()
    for i in range(len(pg)):
        pg_i = pg[i]
        d = df[df.pg==pg_i]
        d_s = d[d.tobacco==1]
        d_ns = d[d.tobacco==0]     
        m_s = np.mean(d_s.dbirwt)
        m_ns = np.mean(d_ns.dbirwt)       
        means_s.append(m_s)
        means_ns.append(m_ns)
    cm = pd.DataFrame({'means_s':means_s, 'means_ns':means_ns, 'pg':pg})
    return cm

# For making scatterplots. y, x must both be array
def scat(x, ys):
    c = ['blue', 'red', 'green', 'yellow', 'purple']
    fig = plt.figure()
    ax = fig.add_subplot(111)
    for i in range(len(ys)):
        ax.scatter(x, ys[i], c=c[i])
    #ax.set_title()
    y=''
    for h in ys: y += str(h.name)+', '   
    ax.set_ylabel(str(y))
    ax.set_xlabel(str(x.name))
    plt.show()
    fig.clf()

def do_g(df):
    # making scatter for 100 group    
    cm_100 = get_cond_means(assign_groups(df, 100))
    scat(cm_100.pg, [cm_100.means_ns, cm_100.means_s])

    # making scatter for 200 group
    cm_200 = get_cond_means(assign_groups(df, 200))
    scat(cm_200.pg, [cm_200.means_ns, cm_200.means_s])

do_g(df)

print("\n\n\n h) Low birth weight births (less than 2500 grams) are considered particularly undesirable since they comprise a large share of infant deaths. Redo g) using an indicator for low birth weight birth as the outcome of interest. Interpret your findings.\n\n\n")

def low_bw_dummy(x):
        if x >= 2500.0:
            return 0 
        elif x < 2500.0:
            return 1                  
df['dbirwt_d'] = df.dbirwt.map(low_bw_dummy)
print(df.dbirwt_d)

# returns small dataframe with p-score groups and dbirwt means for graphing
def get_lbw(df):
    lowbw_s = []
    lowbw_ns = []
    pg = df.pg.unique()
    for i in range(len(pg)):
        pg_i = pg[i]
        d = df[df.pg==pg_i]
        d_s = d[d.tobacco==1]
        d_ns = d[d.tobacco==0]     
        m_s = float(len(d_s[d_s.dbirwt_d ==1])) / float(len(d_s))
        m_ns = float(len(d_ns[d_ns.dbirwt_d ==1])) / float(len(d_ns))
        lowbw_s.append(m_s)
        lowbw_ns.append(m_ns)
    cm = pd.DataFrame({'lowbw_s':lowbw_s, 'lowbw_ns':lowbw_ns, 'pg':pg})
    return cm
      
lbw_100 = get_lbw(assign_groups(df, 100))
#scat(lbw_100.pg, [lbw_100.lowbw_ns, lbw_100.lowbw_s])

lbw_200 = get_lbw(assign_groups(df, 200))
#scat(lbw_200.pg, [lbw_200.lowbw_ns, lbw_200.lowbw_s])

print("\n\n\n i) Estimate the impact of maternal smoking on infant death using the methods in parts a), b), and g) (using 50 equal-sized cells, for smokers and non-smokers together). Interpret your findings. From your results, what might you conclude about the relationship between smoking and infant death? \n\n\n")

# returns small dataframe with p-score groups and death props for graphing
def get_death(df):
    lowbw_s = []
    lowbw_ns = []
    pg = df.pg.unique()
    print(pg)
    for i in range(len(pg)):
        pg_i = pg[i]
        d = df[df.pg==pg_i]
        d_s = d[d.tobacco==1]
        d_ns = d[d.tobacco==0]     
        m_s = float(len(d_s[d_s.death ==1])) / float(len(d_s))
        m_ns = float(len(d_ns[d_ns.death ==1])) / float(len(d_ns))
        lowbw_s.append(m_s)
        lowbw_ns.append(m_ns)
    cm = pd.DataFrame({'death_s':lowbw_s, 'death_ns':lowbw_ns, 'pg':pg})
    return cm

df50 = assign_groups(df, 100)

death_100 = get_death(df50)
#scat(death_100.pg, [death_100.death_ns, death_100.death_s])

print("\n\n\n j) Smoking rates vary over the life-cycle of women. Plot the sample sizes and smoking rates by the age of the women in the sample. Now, separately for smoking and non-smoking women but on the same graph, plot the average birth weight of their infants by the age of the mother. Describe what you see.\n\n\n")

pt = pd.pivot_table(df, ['tobacco'], 'dmage', aggfunc=np.mean)
#scat(pt.index, [pt.tobacco])

pt = pd.pivot_table(df, ['intercept'], 'dmage', aggfunc=np.sum)
#scat(pt.index, [pt.intercept])

df_s = df[df.tobacco==1]
df_ns = df[df.tobacco==0]
pt_s = pd.pivot_table(df_s, ['dbirwt'], 'dmage', aggfunc=np.mean)
pt_ns = pd.pivot_table(df_ns, ['dbirwt'], 'dmage', aggfunc=np.mean)

# scat([pt_ns.index, pt_s.index ], [pt_ns.dbirwt, pt_s.dbirwt])

print("\n\n\n k) For the sample of women aged 16 to 32 (116,243 observations), redo parts a), b), e), f), g) and i).  How do your findings contrast with your findings from the overall sample, if at all?\n\n\n")

df_young = original[(original.dmage<=32)&(original.dmage>=16)]

# parts a, b

do_a(df_young)

do_b(df_young)

# part e)

print(lm_p.summary()) # model built in part e), just printing summary here

print(compare_pg(df_y))

# part f)

do_f(df_y)

# part g)

do_g(df_y)

"""
l) Concisely and coherently summarize all of your findings. In this summary, describe the estimated effects of maternal smoking on infant birth weight and infant mortality and whether you think your “best” estimate of the effects of smoking is credibly identified. State why or why not.

even = pd.Series(df.index).iloc[::2].index

df_train = df[df.index.isin(even)==True]
df_test = df[df.index.isin(even)==False]

dt = DecisionTreeClassifier(criterion='entropy', min_samples_split=40, random_state=99, max_depth=6)
X_train = df_train.loc[:, df_train.columns[df_train.columns.isin(['dbirwt','tobacco','p','pg'])==False]]
X_test = df_test.loc[:, df_test.columns[df_test.columns.isin(['dbirwt','tobacco','p','pg'])==False]]
dt.fit(X_train, df_train.tobacco)

pred = dt.predict_proba(X_test)
p_dt = [e[1] for e in pred]
df_test['p_dt'] = p_dt
sse = np.sum((df_test.tobacco - df_test.p_dt)**2)
print(sse*2) # 14729 w test data (trained on train data)
print("\n\n\n")

scenarios = []
sses = []

rf = RandomForestClassifier(criterion='entropy', min_samples_split=40, n_estimators=100, random_state=99, max_depth=10)
rf.fit(X_train, df_train.tobacco)
pred = rf.predict_proba(X_test)
p_rf = [v[1] for v in pred]

df_test['p_rf'] = p_rf
sse = np.sum((df_test.tobacco - df_test.p_rf)**2)
print(sse*2) # 14461
print("\n\n\n")

lg = sm.Logit(df_train.tobacco, X_train).fit(method='bfgs')

pred = lg.predict(X_test)
df_test['p_lg'] = pred
sse = np.sum((df_test.tobacco - df_test.p)**2)
print(sse*2) # 15043
print("\n\n\n")

# playing w package
X = df.loc[:, df.columns[df.columns.isin(['dbirwt','tobacco'])==False]]
c = CausalModel(df.dbirwt, df.tobacco, X)
#print(c.summary_stats)
"""



